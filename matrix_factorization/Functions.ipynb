{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d245e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from numba import njit\n",
    "\n",
    "import sqlalchemy as db\n",
    "\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db6ae14",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38cac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load the playlist table from the SQL database into the array R_list\n",
    "def make_R_list_sql(conn, pid_limit=None, progress=5., fetch_size=10**6):\n",
    "    if pid_limit is None:\n",
    "        # Get number of pairings\n",
    "        results = conn.execute(db.text(\"SELECT COUNT(*) FROM pairings\"))\n",
    "        N = results.fetchall()[0][0]\n",
    "        \n",
    "        # Get columns one by one\n",
    "        results = conn.execute(db.text(\"SELECT pid, tid FROM pairings\"))\n",
    "    else:\n",
    "        # Get number of pairings\n",
    "        results = conn.execute(db.text(f\"SELECT COUNT(*) FROM pairings WHERE pid<{pid_limit}\"))\n",
    "        N = results.fetchall()[0][0]\n",
    "        \n",
    "        # Get columns one by one\n",
    "        results = conn.execute(db.text(f\"SELECT pid, tid FROM pairings WHERE pid<{pid_limit}\"))\n",
    "    \n",
    "    R_list = np.empty((N,2), dtype=int)\n",
    "    \n",
    "    i = 0\n",
    "    time_0 = time()\n",
    "    show_progress = False\n",
    "    old_i = 0\n",
    "    divisor = np.round(N*progress/100)\n",
    "    while rows:=results.fetchmany(fetch_size):\n",
    "        if progress is not None:\n",
    "            # Show progress\n",
    "            if i/divisor > old_i/divisor:\n",
    "                time_t = time()\n",
    "                print('{:.2f}%: {:.2f} sec'.format(i/N*100, time_t-time_0))\n",
    "                \n",
    "                time_0 = time_t\n",
    "                old_i = i\n",
    "        \n",
    "        for row in rows:\n",
    "            R_list[i,0] = int(row[0])\n",
    "            R_list[i,1] = int(row[1])\n",
    "            i+=1\n",
    "    \n",
    "    return R_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bac83ad",
   "metadata": {},
   "source": [
    "# Data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two dicts such that:\n",
    "# - idx_to_elt gives the i-th element in the sorted list\n",
    "# - elt_to_idx gives the position of elt in the sorted list\n",
    "# We can afford to store this information rather than recomputing it every time\n",
    "def list_to_dict(new_list):\n",
    "    # We only care about unique elements\n",
    "    # Also, np.unique returns the sorted list\n",
    "    new_list = np.unique(new_list)\n",
    "    \n",
    "    elt_to_idx = {}\n",
    "    idx_to_elt = {}\n",
    "    \n",
    "    idx=0\n",
    "    for elt in new_list:\n",
    "        idx_to_elt[idx] = elt\n",
    "        elt_to_idx[elt] = idx\n",
    "        \n",
    "        idx+=1\n",
    "    \n",
    "    return elt_to_idx, idx_to_elt\n",
    "\n",
    "# Replaces each entry of R_list with its indices as given by pid_to_idx and tid_to_idx\n",
    "def array_to_idx(R_list, pid_to_idx, tid_to_idx):\n",
    "    R_list_idx = np.empty( R_list.shape, dtype=int)\n",
    "    \n",
    "    for row in range(R_list.shape[0]):\n",
    "        R_list_idx[row,0] = pid_to_idx[ R_list[row,0] ]\n",
    "        R_list_idx[row,1] = tid_to_idx[ R_list[row,1] ]\n",
    "    \n",
    "    return R_list_idx\n",
    "\n",
    "# Format an unseen R_list to be processed in our pipeline\n",
    "# We can only process songs that are in tid_to_idx already, so we remove those we don't know\n",
    "def format_new_R_list(R_list_new, tid_to_idx):\n",
    "    # Remove tracks that we don't recognize\n",
    "    tid_known = list(tid_to_idx.keys())\n",
    "    R_list_new = R_list_new[ np.isin(R_list_new[:,1], tid_known), : ]\n",
    "    \n",
    "    pid_to_idx, idx_to_pid = list_to_dict(R_list_new[:,0])\n",
    "    \n",
    "    R_idx_new = array_to_idx(R_list_new, pid_to_idx, tid_to_idx)\n",
    "    \n",
    "    return R_list_new, R_idx_new, pid_to_idx, idx_to_pid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5f8785",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e31dd1",
   "metadata": {},
   "source": [
    "The functions for running gradient descent and computing the error function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an epoch of gradient descent where the iterations parameter is the number of iterations.\n",
    "@njit\n",
    "def run_epoch(R_idx_list, P, Q, alpha, llambda, iterations):\n",
    "    oldP = P.copy()\n",
    "    oldQ = Q.copy()\n",
    "    f = np.shape(P)[1]\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        newP = oldP.copy()\n",
    "        newQ = oldQ.copy()\n",
    "        for u,i in R_idx_list:\n",
    "            dotprod = 0\n",
    "            for feature in range(f):\n",
    "                dotprod += oldP[u,feature] * oldQ[i,feature]\n",
    "            error = dotprod - 1\n",
    "\n",
    "            for feature in range(f):\n",
    "                pf = oldP[u,feature]\n",
    "                qf = oldQ[i,feature]\n",
    "                newP[u,feature] -= alpha * (error * qf + llambda * pf) \n",
    "                newQ[i,feature] -= alpha * (error * pf + llambda * qf)\n",
    "        oldP = newP\n",
    "        oldQ = newQ\n",
    "    return newP, newQ\n",
    "\n",
    "@njit\n",
    "def SqError(R_list, P, Q):\n",
    "    result = 0\n",
    "    \n",
    "    # sum over R_list\n",
    "    for pid, tid in R_list:\n",
    "        result += (1 - P[pid,:]@Q[tid,:])**2\n",
    "    \n",
    "    return result\n",
    "\n",
    "@njit\n",
    "def MSE(R_list, P, Q):\n",
    "    result = 0\n",
    "    N = R_list.shape[0]\n",
    "    \n",
    "    # sum over R_list\n",
    "    for pid, tid in R_list:\n",
    "        result += (1 - P[pid,:]@Q[tid,:])**2\n",
    "    \n",
    "    return result/N\n",
    "\n",
    "@njit\n",
    "# MSE with L2 regularization\n",
    "def error_function_l2(R_list, P , Q, llambda):\n",
    "    result = 0\n",
    "    \n",
    "    #sum over R_list\n",
    "    for row, col in R_list:\n",
    "        result += (1 - P[row,:]@Q[col,:])**2\n",
    "    \n",
    "    result += llambda * (np.linalg.norm(P)**2 + np.linalg.norm(Q)**2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd9313",
   "metadata": {},
   "source": [
    "The function for obtaining the playlist vector that minimizes the cost function for a fixed $Q$ from a list of track id's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02722b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_user_vec(Y, llambda):\n",
    "    d,f = np.shape(Y)\n",
    "    vec = np.linalg.inv(np.transpose(Y) @ Y + llambda * np.identity(f)) @ np.transpose(Y) @ np.ones((d,1))\n",
    "    return np.transpose(vec)\n",
    "\n",
    "def make_Pval(R_list_new, Q, llambda):\n",
    "    _, f = np.shape(Q)\n",
    "    \n",
    "    new_pids = np.unique(R_list_new[:,0])\n",
    "    P_val = np.zeros((len(new_pids), f))\n",
    "    \n",
    "    for pid in new_pids:\n",
    "        # get list of tracks in the playlist\n",
    "        tid_list = R_list_new[ R_list_new[:,0]==pid, 1]\n",
    "\n",
    "        # x is the row of Pval corresponding to this pid\n",
    "        x = new_user_vec(Q[tid_list,:], llambda)\n",
    "        \n",
    "        for feature in range(f):\n",
    "            P_val[pid, feature] = x[0,feature]\n",
    "        \n",
    "    return P_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c9860",
   "metadata": {},
   "source": [
    "# Choosing hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaa232d",
   "metadata": {},
   "source": [
    "Define functions to try generic combinations of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use grid search to compute the validation cost for all combinations of values of f, alpha, and llambda\n",
    "def grid_search(f_values, alpha_values, llambda_values, NUM_ITERATIONS, path='models'):\n",
    "    nF = len(f_values)\n",
    "    nA = len(alpha_values)\n",
    "    nL = len(llambda_values)\n",
    "    costs = np.zeros((nF,nA,nL))\n",
    "    \n",
    "    # Create folder to save trained models if it does not exist already\n",
    "    pathlib.Path(path).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Execute grid search\n",
    "    num_file = 0\n",
    "    time_start = time()\n",
    "    for idf in range(nF):\n",
    "        f = f_values[idf]\n",
    "        for ida in range(nA):\n",
    "            alpha = alpha_values[ida]\n",
    "            for idl in range(nL):\n",
    "                llambda = llambda_values[idl]\n",
    "\n",
    "                time_0 = time()\n",
    "                iter_count = nA*nL*idf + nL*ida + idl+1\n",
    "                print(f'Processing {iter_count}/{nF*nA*nL}:')\n",
    "                print(f'({idf+1},{ida+1},{idl+1})/({nF},{nA},{nL})')\n",
    "                error = False\n",
    "\n",
    "                # Initialize random values\n",
    "                P_initial = np.random.normal(0, 0.1, (num_playlists, f))\n",
    "                Q_initial = np.random.normal(0, 0.1, (num_songs, f))\n",
    "                \n",
    "                # Obtain P, Q with the chosen hyperparameters\n",
    "                P_trained, Q_trained = run_epoch(R_idx_train, P=P_initial, Q=Q_initial, alpha=alpha, llambda=llambda, iterations=NUM_ITERATIONS)\n",
    "                \n",
    "                # Skip iteration if Q_trained contains a nan\n",
    "                if np.isnan(Q_trained).any():\n",
    "                    costs[idf,ida,idl] = np.nan\n",
    "                    error = True\n",
    "                else:\n",
    "                    # Check results against the validation set\n",
    "                    P_val = make_Pval(R_idx_val, Q_trained, llambda)\n",
    "                    costs[idf,ida,idl] = MSE(R_idx_val, P_val, Q_trained)\n",
    "                \n",
    "                # Show progress\n",
    "                time_t = time()\n",
    "                print('Training:  ', end=' ')\n",
    "                print(str( timedelta(seconds=time_t-time_0) )[:-4])\n",
    "                \n",
    "                time_0 = time()\n",
    "                file_name = f'{path}/Q_trained_{num_file}'\n",
    "                np.save(file_name, Q_trained)\n",
    "                num_file += 1\n",
    "                time_t = time()\n",
    "                \n",
    "                print('Saving:    ', end=' ')\n",
    "                print(str( timedelta(seconds=time_t-time_0) )[:-4])\n",
    "                \n",
    "                print('Total time:', end=' ')\n",
    "                print(str( timedelta(seconds=time_t-time_start) )[:-4])\n",
    "                \n",
    "                if error:\n",
    "                    print('-- Failed -- found nan')\n",
    "                print()\n",
    "    \n",
    "    return costs\n",
    "\n",
    "# Plot the errors found by grid_search\n",
    "def plot_grid_search(costs, f_values, alpha_values, llambda_values):\n",
    "    nF = len(f_values)\n",
    "    nA = len(alpha_values)\n",
    "    nL = len(llambda_values)\n",
    "    \n",
    "    # Plot results\n",
    "    for ida in range(nA):\n",
    "        for idl in range(nL):\n",
    "            alpha = '{:.1E}'.format(alpha_values[ida])\n",
    "            llambda = '{:.1E}'.format(llambda_values[idl])\n",
    "\n",
    "            cost_f = costs[:,ida,idl]\n",
    "            plt.plot(f_values, cost_f, '.', label=(alpha,llambda))\n",
    "            \n",
    "            \"\"\"\n",
    "            idt = 0\n",
    "            display = True\n",
    "            while np.isnan(cost_f[idt]):\n",
    "                idt += 1\n",
    "                if idt == nL:\n",
    "                    display = False\n",
    "                    break\n",
    "            if display:\n",
    "                plt.text(f_values[0], cost_f[0], str((alpha,llambda)))\n",
    "            \"\"\"\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1,0.5))\n",
    "    plt.xlabel('Number of latent features')\n",
    "\n",
    "    print('Additional parameters:')\n",
    "    print('Alpha values:', alpha_values)\n",
    "    print('llambda values:', llambda_values)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
