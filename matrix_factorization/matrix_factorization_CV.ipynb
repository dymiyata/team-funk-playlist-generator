{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dymiyata/erdos2023_million_playlist_challenge/blob/master/matrix_factorization/matrix_factorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDkLq8hzIEeE",
    "outputId": "5f75df2d-a2ff-410d-a2c4-1344180c2c3d"
   },
   "outputs": [],
   "source": [
    "print(\"Testing Colab and Github integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JZtDAeafHyXf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "from numba import njit\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions\n",
    "\n",
    "First we define a function to read in the first n json files of data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# n is the number of json files we wish to read (max is 1000)\n",
    "def read_data(n):\n",
    "    for i in range(n):\n",
    "        # create the file_name string in order to read in file\n",
    "        file_name = '../spotify_million_playlist_dataset/data/mpd.slice.' \\\n",
    "            + str(i*1000) \\\n",
    "            + '-' + str(i*1000+999) + '.json'\n",
    "\n",
    "        # Uncomment the following line to show progress\n",
    "        # print(file_name)\n",
    "\n",
    "        # open the file and store its contents in file_contents\n",
    "        with open(file_name) as user_file:\n",
    "            file_contents = user_file.read()\n",
    "\n",
    "        # we only care about the \"playlists\" part of this dictionary\n",
    "        # save the list of playlists in playlist_list\n",
    "        parsed_json = json.loads(file_contents)\n",
    "        playlist_list = parsed_json[\"playlists\"]\n",
    "\n",
    "\n",
    "        # create dataframe if it's first playlist, otherwise append info to existing dataframe\n",
    "        # the record_path argument tells the json_normalize function how to flatten the data\n",
    "        # the meta argument tells the json_nomralize function what meta data to keep\n",
    "        if i == 0:\n",
    "            data = pd.json_normalize(\n",
    "                playlist_list,\n",
    "                record_path = \"tracks\",\n",
    "                meta = [\"name\", \"collaborative\", \"pid\", \"num_followers\", \"num_edits\"]\n",
    "            )\n",
    "        else:\n",
    "            data = pd.concat(\n",
    "                [\n",
    "                    data,\n",
    "                    pd.json_normalize(\n",
    "                        playlist_list,\n",
    "                        record_path = \"tracks\",\n",
    "                        meta = [\"name\", \"collaborative\", \"pid\", \"num_followers\", \"num_edits\"]\n",
    "                    )\n",
    "                ],\n",
    "                ignore_index = True\n",
    "            )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define functions for creating relevant dictionaries and lists from our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fPke0KhzHyXh"
   },
   "outputs": [],
   "source": [
    "\n",
    "# dictionary to translate track uri's to integer indices\n",
    "def make_track_dict(df):\n",
    "    result = {}\n",
    "    i = 0\n",
    "    for uri in pd.unique(df['track_uri']):\n",
    "        result[uri] = i\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "# dictionary to translate integer indices to track uri's\n",
    "def make_reverse_track_dict(df):\n",
    "    result = {}\n",
    "    i = 0\n",
    "    for uri in pd.unique(df['track_uri']):\n",
    "        result[i] = uri\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "# dictionary to translate from tid to track name\n",
    "def make_index_to_track_dict(df):\n",
    "    result = {}\n",
    "    i = 0\n",
    "    for uri in pd.unique(df['track_uri']):\n",
    "        name = df.query(\"track_uri == @uri\").iloc[0][\"track_name\"]\n",
    "        result[i] = name\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "# dictionary to translate pid to integer indices\n",
    "def make_pid_dict(df):\n",
    "    result = {}\n",
    "    i = 0\n",
    "    for pid in pd.unique(df['pid']):\n",
    "        result[pid] = i\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "# get list of tid's sorted in order of how many times song appears (highest to lowest)\n",
    "def make_sorted_tid_list(R_list):\n",
    "    # sort R_list by second entry\n",
    "    R_list_sorted = copy.deepcopy(R_list)\n",
    "    R_list_sorted.sort(key = lambda x : x[1])\n",
    "\n",
    "    # create song_count_dict whose keys are tid's and whose values are the number of playlists the given key appears in\n",
    "    song_count_dict = {}\n",
    "    tid_current = 0\n",
    "    song_count_dict[0] = 0\n",
    "    for pid, tid in R_list_sorted:\n",
    "        if tid == tid_current:\n",
    "            song_count_dict[tid] += 1\n",
    "        else:\n",
    "            tid_current = tid\n",
    "            song_count_dict[tid] = 1\n",
    "\n",
    "    # result is a list of keys of song_count_dict sorted by their value (highest to lowest)\n",
    "    result = sorted(song_count_dict, key = song_count_dict.get)[::-1]\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions that convert the data stored in our dataframe to forms that will be used by our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create the big array R consisting of 1's and 0's. I didn't end up using this\n",
    "def make_R(df):\n",
    "    m = len(pd.unique(df['pid']))\n",
    "    n = len(pd.unique(df['track_uri']))\n",
    "    track_dict = make_track_dict(df)\n",
    "    pid_dict = make_pid_dict(df)\n",
    "\n",
    "    result = np.zeros([m,n])\n",
    "    for index , row in df.iterrows():\n",
    "        result[pid_dict[row['pid']], track_dict[row['track_uri']]] = 1\n",
    "    return result\n",
    "\n",
    "\n",
    "# create list of tuples where recommendation array should have a 1\n",
    "# i.e. if (2,18) is in this list, then playlist 2 contains track 18\n",
    "def make_R_list(df):\n",
    "    result = []\n",
    "    track_dict = make_track_dict(df)\n",
    "    pid_dict = make_pid_dict(df)\n",
    "    for index , row in df.iterrows():\n",
    "        result.append((pid_dict[row['pid']], track_dict[row['track_uri']]))\n",
    "    return np.array(result)\n",
    "\n",
    "# create the R_list for grandma's hypothesis method.  As coded top 20% gets 2 instead of 1.\n",
    "def make_R_list_grandma(df):\n",
    "    result = []\n",
    "    track_dict = make_track_dict(df)\n",
    "    for index , row in df.iterrows():\n",
    "        if row['pos'] < 0.2 * row['num_tracks']:\n",
    "            result.append((row['pid'], track_dict[row['track_uri']], 2))\n",
    "        else: \n",
    "            result.append((row['pid'], track_dict[row['track_uri']], 1))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions for running gradient descent and computing the error function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use gradient descent to minimize MSE (with l2 regularization)\n",
    "def update_params_loop(R_list, P, Q, alpha, llambda):\n",
    "    newP = P\n",
    "    newQ = Q\n",
    "    m , f = np.shape(P)\n",
    "    n = np.shape(Q)[0]\n",
    "\n",
    "    for u,i in R_list:\n",
    "        newP[u,:] -= alpha * 2 * (P[u,:] @ Q[i,:] - 1) * Q[i,:]\n",
    "        newQ[i,:] -= alpha * 2 * (P[u,:] @ Q[i,:] - 1) * P[u,:]\n",
    "    for u in range(m):\n",
    "        newP[u,:] -= alpha * 2 * llambda * P[u,:]\n",
    "    for i in range(n):\n",
    "        newQ[i,:] -= alpha * 2 * llambda * Q[i,:]\n",
    "    return (newP, newQ)\n",
    "\n",
    "# use gradient descent with where R_list has triples (u,i,score)\n",
    "def update_params_loop_score(R_list, P, Q, alpha, llambda):\n",
    "    newP = P\n",
    "    newQ = Q\n",
    "    m , f = np.shape(P)\n",
    "    n = np.shape(Q)[0]\n",
    "\n",
    "    for u,i, score in R_list:\n",
    "        newP[u,:] -= alpha * 2 * (P[u,:] @ Q[i,:] - score) * Q[i,:]\n",
    "        newQ[i,:] -= alpha * 2 * (P[u,:] @ Q[i,:] - score) * P[u,:]\n",
    "    for u in range(m):\n",
    "        newP[u,:] -= alpha * 2 * llambda * P[u,:]\n",
    "    for i in range(n):\n",
    "        newQ[i,:] -= alpha * 2 * llambda * Q[i,:]\n",
    "    return (newP, newQ)\n",
    "\n",
    "# Run an epoch of gradient descent where the iterations parameter is the number of iterations.\n",
    "@njit\n",
    "def run_epoch(R_array, P, Q, alpha, llambda, iterations):\n",
    "    oldP = P.copy()\n",
    "    oldQ = Q.copy()\n",
    "    f = np.shape(P)[1]\n",
    "\n",
    "    for iter in range(iterations):\n",
    "        newP = oldP.copy()\n",
    "        newQ = oldQ.copy()\n",
    "        for u,i in R_array:\n",
    "            dotprod = 0\n",
    "            for feature in range(f):\n",
    "                dotprod += oldP[u,feature] * oldQ[i,feature]\n",
    "            error = dotprod - 1\n",
    "\n",
    "            for feature in range(f):\n",
    "                pf = oldP[u,feature]\n",
    "                qf = oldQ[i,feature]\n",
    "                newP[u,feature] -= alpha * (error * qf + llambda * pf) \n",
    "                newQ[i,feature] -= alpha * (error * pf + llambda * qf)\n",
    "        oldP = newP\n",
    "        oldQ = newQ\n",
    "    return newP, newQ\n",
    "\n",
    "\n",
    "#runs the gradient descent loop in batches\n",
    "def gd_batch(R_list, P, Q, alpha, llambda, batch_num, iterations, R = None, verbose=False):\n",
    "    #make copies of P and Q\n",
    "    P_current = P.copy()\n",
    "    Q_current = Q.copy()\n",
    "\n",
    "    #shuffle R_list\n",
    "    #divide R_list into batch_num subsets\n",
    "    random.shuffle(R_list)\n",
    "    batch_size = int(np.ceil(len(R_list)/batch_num))\n",
    "    R_batch = [R_list[i:i+batch_size] for i in range(0,len(R_list), batch_size) ]\n",
    "\n",
    "    #loop over total iterations\n",
    "    for i in range(iterations):\n",
    "        #if verbose == true print out error function\n",
    "        if verbose:\n",
    "            print(f'Step {i*batch_num}: Error function={error_function(R_list,R , P_current, Q_current)}')\n",
    "        #loop over batch_num\n",
    "        for batch in R_batch:\n",
    "            #run update_param_loop on batch\n",
    "            P_current , Q_current = update_params_loop(batch, P_current, Q_current, alpha, llambda)\n",
    "\n",
    "    return (P_current , Q_current)\n",
    "\n",
    "#error function without l2 normalization factor\n",
    "def error_function( R_list,R, P , Q ):\n",
    "    result = 0\n",
    "    #sum over R_list\n",
    "    for row, col in R_list:\n",
    "        result = result + (R[row,col] - P[row,:]@Q[col,:])**2\n",
    "\n",
    "    return result\n",
    "\n",
    "@njit\n",
    "def error_function_l2( R_list, P , Q, llambda):\n",
    "    result = 0\n",
    "    #sum over R_list\n",
    "    for row, col in R_list:\n",
    "        result = result + (1 - P[row,:]@Q[col,:])**2\n",
    "    result += llambda * (np.linalg.norm(P)**2 + np.linalg.norm(Q)**2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function for obtaining the playlist vector that minimizes the cost function for a fixed $Q$ from a list of track id's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_user_vec(tid_list, Q, llambda):\n",
    "    Y = Q[tid_list,:]\n",
    "    f = np.shape(Q)[1]\n",
    "    d = len(tid_list)\n",
    "    vec = np.linalg.inv(np.transpose(Y) @ Y + llambda * np.identity(f)) @ np.transpose(Y) @ np.ones((d,1))\n",
    "    return np.transpose(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRfZHGotHyXh"
   },
   "source": [
    "# Example of training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells read in data and creates the list of data points.  Ideally this will be done by querying the SQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "YhJsEpFdHyXi"
   },
   "outputs": [],
   "source": [
    "num_jsons = 6 # number of files to read\n",
    "num_playlists = num_jsons*1000\n",
    "data = read_data(num_jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_list = make_R_list(data)\n",
    "track_dict = make_track_dict(data)\n",
    "reverse_track_dict = make_reverse_track_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]\n",
      "[75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "I_train, I_test = train_test_split(range(100), test_size=0.25, shuffle=False)\n",
    "print(I_train)\n",
    "print(I_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of the total database to reserve for validation and testing\n",
    "val_size_abs = 0.25\n",
    "test_size    = 0.25\n",
    "shuffle = False\n",
    "\n",
    "# Note: the first pid_train contains (1-test_size) percent of the data.\n",
    "# We need to use val_size so that val_size*(1-test_size) = val_size_abs.\n",
    "val_size = val_size_abs/(1-test_size)\n",
    "pid_train, pid_test = train_test_split(range(num_playlists), test_size=test_size, shuffle=shuffle)\n",
    "pid_train, pid_val  = train_test_split(pid_train, test_size=val_size, shuffle=shuffle)\n",
    "\n",
    "R_list_train = R_list[ np.isin(R_list[:,0], pid_train), :]\n",
    "R_list_val   = R_list[ np.isin(R_list[:,0], pid_val),   :]\n",
    "R_list_test  = R_list[ np.isin(R_list[:,0], pid_test),  :]\n",
    "\n",
    "# Store the track id of songs in the train/val/test sets\n",
    "tid_train = list(np.unique( R_list_train[:,1] ))\n",
    "tid_val   = list(np.unique( R_list_val[:,1]   ))\n",
    "tid_test  = list(np.unique( R_list_test[:,1]  ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify the number of features and create matrices $P$ and $Q$ whose entries are randomly taken from a normal distribution with $\\mu = 0$ and $\\sigma = 0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 10 # number of latent features\n",
    "# num_songs = max(reverse_track_dict.keys()) + 1\n",
    "num_songs = len(tid_train)\n",
    "num_playlists = len(pid_train)\n",
    "\n",
    "# initialize random values for matrices P and Q. Entries are between -1 and 1\n",
    "P = np.random.normal(0, 0.1, (num_playlists, f))\n",
    "Q = np.random.normal(0, 0.1, (num_songs, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we run the gradient descent algorithm and store the resulting matrices in P_trained and Q_trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "b_nyoRWcHyXi"
   },
   "outputs": [],
   "source": [
    "# Run gradient descent algorithm with alpha = 0.001, llambda = 0.005 for 100 iterations\n",
    "P_trained, Q_trained = run_epoch(R_list_train, P, Q, 0.001, 0.005, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of computing error on non-training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to demonstrate how to compute error for a new collection of playlists, let's first read in an extra json file's worth of data (I could've just read in the single extra file instead of all original files again but I was too lazy to change the read_data function). Again this step should utilize the SQL database.\n",
    "\n",
    "**Note:** I changed R_list_new into R_list_test during the train-val-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of data points corresponding to our extra json file\n",
    "# R_list_new = np.array([(u,i) for u,i in make_R_list(data1) if u >= num_jsons*1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our fixed Q, given a list of (new) playlist ids, we can compute the P matrix that minimizes the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS FUNCTION, (as well as new_user_vec) CAN AND SHOULD BE OPTIMIZED TO USE numba.  \n",
    "# For the sake of time, I'll leave it as is for now\n",
    "# If we can avoid using lists (like tid_list), then things will be numba compatible. \n",
    "def make_Pval(new_pids, R_list_new, Q, llambda, tid_known=tid_train):\n",
    "    num_songs , f = np.shape(Q)\n",
    "    P_val = np.zeros((len(new_pids), f))\n",
    "    count = 0 # keeps track of where in pid_list we are\n",
    "    \n",
    "    for pid in new_pids:\n",
    "        # create list of tracks in the playlist\n",
    "        # Remember: R_list_new has two columns: 0 is pid, 1 is tid\n",
    "        tid_list = R_list_new[ R_list_new[:,0]==pid, 1]\n",
    "        \n",
    "        # With repetition\n",
    "        tid_list = list(tid_list)\n",
    "        tid_list = [tid for tid in tid_list if tid in tid_known]\n",
    "        \n",
    "        # Without repetition\n",
    "        # tid_list = list( set(tid_list).intersection(tid_known) )\n",
    "\n",
    "        # x is the row of Pval corresponding to this pid\n",
    "        x = new_user_vec(tid_list, Q_trained, llambda)\n",
    "        for feature in range(f):\n",
    "            P_val[count, feature] = x[0,feature]\n",
    "        count += 1\n",
    "        \n",
    "    return P_val\n",
    "\n",
    "Pval2 = make_Pval(pid_test, R_list_test, Q_trained, 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the error function on this set. Note, we can't use the function defined at the top of this document because now the pid does not correspond to the index of the column of Pval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56951.27274909439"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def val_error(R_list_new, pid_list, Pval, Q, llambda):\n",
    "    num_songs, f = np.shape(Q)\n",
    "    result = 0\n",
    "    for pid, tid in R_list_new:\n",
    "        # As in the make_Pval function, the following condition needs to be changed for a randomized val set.\n",
    "        if tid < num_songs:\n",
    "            result += (1 - Pval[pid_list.index(pid), :] @ Q[tid, :])\n",
    "    result += llambda * (np.linalg.norm(Pval)**2)\n",
    "    return result\n",
    "\n",
    "val_error(R_list_test, pid_test, Pval, Q_trained, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.39'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlalchemy as db\n",
    "db.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
