{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dymiyata/erdos2023_million_playlist_challenge/blob/master/matrix_factorization/matrix_factorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDkLq8hzIEeE",
    "outputId": "5f75df2d-a2ff-410d-a2c4-1344180c2c3d"
   },
   "outputs": [],
   "source": [
    "print(\"Testing Colab and Github integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JZtDAeafHyXf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old functions\n",
    "I'm keeping old code here here in case we need it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gradient descent to minimize MSE (with l2 regularization)\n",
    "def update_params_loop(R_list, P, Q, alpha, llambda):\n",
    "    newP = P\n",
    "    newQ = Q\n",
    "    m , f = np.shape(P)\n",
    "    n = np.shape(Q)[0]\n",
    "\n",
    "    for u,i in R_list:\n",
    "        newP[u,:] -= alpha * 2 * (P[u,:] @ Q[i,:] - 1) * Q[i,:]\n",
    "        newQ[i,:] -= alpha * 2 * (P[u,:] @ Q[i,:] - 1) * P[u,:]\n",
    "    for u in range(m):\n",
    "        newP[u,:] -= alpha * 2 * llambda * P[u,:]\n",
    "    for i in range(n):\n",
    "        newQ[i,:] -= alpha * 2 * llambda * Q[i,:]\n",
    "    return (newP, newQ)\n",
    "\n",
    "# use gradient descent with where R_list has triples (u,i,score)\n",
    "def update_params_loop_score(R_list, P, Q, alpha, llambda):\n",
    "    newP = P\n",
    "    newQ = Q\n",
    "    m , f = np.shape(P)\n",
    "    n = np.shape(Q)[0]\n",
    "\n",
    "    for u,i, score in R_list:\n",
    "        newP[u,:] -= alpha * 2 * (P[u,:] @ Q[i,:] - score) * Q[i,:]\n",
    "        newQ[i,:] -= alpha * 2 * (P[u,:] @ Q[i,:] - score) * P[u,:]\n",
    "    for u in range(m):\n",
    "        newP[u,:] -= alpha * 2 * llambda * P[u,:]\n",
    "    for i in range(n):\n",
    "        newQ[i,:] -= alpha * 2 * llambda * Q[i,:]\n",
    "    return (newP, newQ)\n",
    "\n",
    "#runs the gradient descent loop in batches\n",
    "def gd_batch(R_list, P, Q, alpha, llambda, batch_num, iterations, R = None, verbose=False):\n",
    "    #make copies of P and Q\n",
    "    P_current = P.copy()\n",
    "    Q_current = Q.copy()\n",
    "\n",
    "    #shuffle R_list\n",
    "    #divide R_list into batch_num subsets\n",
    "    random.shuffle(R_list)\n",
    "    batch_size = int(np.ceil(len(R_list)/batch_num))\n",
    "    R_batch = [R_list[i:i+batch_size] for i in range(0,len(R_list), batch_size) ]\n",
    "\n",
    "    #loop over total iterations\n",
    "    for i in range(iterations):\n",
    "        #if verbose == true print out error function\n",
    "        if verbose:\n",
    "            print(f'Step {i*batch_num}: Error function={error_function(R_list,R , P_current, Q_current)}')\n",
    "        #loop over batch_num\n",
    "        for batch in R_batch:\n",
    "            #run update_param_loop on batch\n",
    "            P_current , Q_current = update_params_loop(batch, P_current, Q_current, alpha, llambda)\n",
    "\n",
    "    return (P_current , Q_current)\n",
    "\n",
    "#error function without l2 normalization factor\n",
    "def error_function( R_list,R, P , Q ):\n",
    "    result = 0\n",
    "    #sum over R_list\n",
    "    for row, col in R_list:\n",
    "        result = result + (R[row,col] - P[row,:]@Q[col,:])**2\n",
    "\n",
    "    return result\n",
    "\n",
    "# Old validation result\n",
    "def val_error(R_list_new, Pval, Q, llambda):\n",
    "    result = 0\n",
    "    \n",
    "    _, f = np.shape(Q)\n",
    "    result = 0\n",
    "    \n",
    "    for pid, tid in R_list_new:\n",
    "        result += (1 - Pval[pid, :] @ Q[tid, :])**2\n",
    "    \n",
    "    result += llambda * (np.linalg.norm(Pval)**2 + np.linalg.norm(Q)**2)\n",
    "    return result\n",
    "\n",
    "def new_user_vec_old(tid_list, Q, llambda):\n",
    "    Y = Q[tid_list,:]\n",
    "    f = np.shape(Q)[1]\n",
    "    d = len(tid_list)\n",
    "    vec = np.linalg.inv(np.transpose(Y) @ Y + llambda * np.identity(f)) @ np.transpose(Y) @ np.ones((d,1))\n",
    "    return np.transpose(vec)\n",
    "\n",
    "def make_Pval_old(R_list_new, Q, llambda):\n",
    "    _, f = np.shape(Q)\n",
    "    \n",
    "    new_pids = np.unique(R_list_new[:,0])\n",
    "    P_val = np.zeros((len(new_pids), f))\n",
    "    \n",
    "    for pid in new_pids:\n",
    "        # get list of tracks in the playlist\n",
    "        tid_list = R_list_new[ R_list_new[:,0]==pid, 1]\n",
    "\n",
    "        # x is the row of Pval corresponding to this pid\n",
    "        x = new_user_vec_old(tid_list, Q, llambda)\n",
    "        \n",
    "        for feature in range(f):\n",
    "            P_val[pid, feature] = x[0,feature]\n",
    "        \n",
    "    return P_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next code to test that `make_Pval` and `make_Pval_old` give the same answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradient descent algorithm with alpha = 0.001, llambda = 0.005 for 100 iterations\n",
    "start_time = time()\n",
    "P_val = make_Pval(R_idx_val, Q_trained, 0.005)\n",
    "print(MSE(R_idx_val, P_val, Q_trained))\n",
    "end_time = time()\n",
    "print('Done (numba): {:.3f} sec'.format(end_time-start_time))\n",
    "print()\n",
    "\n",
    "# Run gradient descent algorithm with alpha = 0.001, llambda = 0.005 for 100 iterations\n",
    "start_time = time()\n",
    "P_val_old = make_Pval_old(R_idx_val, Q_trained, 0.005)\n",
    "print(MSE(R_idx_val, P_val_old, Q_trained))\n",
    "end_time = time()\n",
    "print('Done (old): {:.3f} sec'.format(end_time-start_time))\n",
    "print()\n",
    "\n",
    "print('Same answer:', np.isclose(P_val, P_val_old).all())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
