{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dymiyata/erdos2023_million_playlist_challenge/blob/master/matrix_factorization/matrix_factorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDkLq8hzIEeE",
    "outputId": "5f75df2d-a2ff-410d-a2c4-1344180c2c3d"
   },
   "outputs": [],
   "source": [
    "print(\"Testing Colab and Github integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JZtDAeafHyXf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions\n",
    "\n",
    "First we define a function to read in the first n json files of data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# n is the number of json files we wish to read (max is 1000)\n",
    "def read_data(n):\n",
    "    for i in range(n):\n",
    "        # create the file_name string in order to read in file\n",
    "        file_name = '../spotify_million_playlist_dataset/data/mpd.slice.' \\\n",
    "            + str(i*1000) \\\n",
    "            + '-' + str(i*1000+999) + '.json'\n",
    "\n",
    "        # Uncomment the following line to show progress\n",
    "        # print(file_name)\n",
    "\n",
    "        # open the file and store its contents in file_contents\n",
    "        with open(file_name) as user_file:\n",
    "            file_contents = user_file.read()\n",
    "\n",
    "        # we only care about the \"playlists\" part of this dictionary\n",
    "        # save the list of playlists in playlist_list\n",
    "        parsed_json = json.loads(file_contents)\n",
    "        playlist_list = parsed_json[\"playlists\"]\n",
    "\n",
    "\n",
    "        # create dataframe if it's first playlist, otherwise append info to existing dataframe\n",
    "        # the record_path argument tells the json_normalize function how to flatten the data\n",
    "        # the meta argument tells the json_nomralize function what meta data to keep\n",
    "        if i == 0:\n",
    "            data = pd.json_normalize(\n",
    "                playlist_list,\n",
    "                record_path = \"tracks\",\n",
    "                meta = [\"name\", \"collaborative\", \"pid\", \"num_followers\", \"num_edits\"]\n",
    "            )\n",
    "        else:\n",
    "            data = pd.concat(\n",
    "                [\n",
    "                    data,\n",
    "                    pd.json_normalize(\n",
    "                        playlist_list,\n",
    "                        record_path = \"tracks\",\n",
    "                        meta = [\"name\", \"collaborative\", \"pid\", \"num_followers\", \"num_edits\"]\n",
    "                    )\n",
    "                ],\n",
    "                ignore_index = True\n",
    "            )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define functions for creating relevant dictionaries and lists from our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "fPke0KhzHyXh"
   },
   "outputs": [],
   "source": [
    "\n",
    "# dictionary to translate track uri's to integer indices\n",
    "def make_track_dict(df):\n",
    "    result = {}\n",
    "    i = 0\n",
    "    for uri in pd.unique(df['track_uri']):\n",
    "        result[uri] = i\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "# dictionary to translate track uri's to integer indices\n",
    "def make_reverse_track_dict(df):\n",
    "    result = {}\n",
    "    i = 0\n",
    "    for uri in pd.unique(df['track_uri']):\n",
    "        result[i] = uri\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "# dictionary to translate from tid to track name\n",
    "def make_index_to_track_dict(df):\n",
    "    result = {}\n",
    "    i = 0\n",
    "    for uri in pd.unique(df['track_uri']):\n",
    "        name = df.query(\"track_uri == @uri\").iloc[0][\"track_name\"]\n",
    "        result[i] = name\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "# dictionary to translate pid to integer indices\n",
    "def make_pid_dict(df):\n",
    "    result = {}\n",
    "    i = 0\n",
    "    for pid in pd.unique(df['pid']):\n",
    "        result[pid] = i\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "# get list of tid's sorted in order of how many times song appears (highest to lowest)\n",
    "def make_sorted_tid_list(R_list):\n",
    "    # sort R_list by second entry\n",
    "    R_list_sorted = copy.deepcopy(R_list)\n",
    "    R_list_sorted.sort(key = lambda x : x[1])\n",
    "\n",
    "    # create song_count_dict whose keys are tid's and whose values are the number of playlists the given key appears in\n",
    "    song_count_dict = {}\n",
    "    tid_current = 0\n",
    "    song_count_dict[0] = 0\n",
    "    for pid, tid in R_list_sorted:\n",
    "        if tid == tid_current:\n",
    "            song_count_dict[tid] += 1\n",
    "        else:\n",
    "            tid_current = tid\n",
    "            song_count_dict[tid] = 1\n",
    "\n",
    "    # result is a list of keys of song_count_dict sorted by their value (highest to lowest)\n",
    "    result = sorted(song_count_dict, key = song_count_dict.get)[::-1]\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions that convert the data stored in our dataframe to forms that will be used by our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create the big array R consisting of 1's and 0's. I didn't end up using this\n",
    "def make_R(df):\n",
    "    m = len(pd.unique(df['pid']))\n",
    "    n = len(pd.unique(df['track_uri']))\n",
    "    track_dict = make_track_dict(df)\n",
    "    pid_dict = make_pid_dict(df)\n",
    "\n",
    "    result = np.zeros([m,n])\n",
    "    for index , row in df.iterrows():\n",
    "        result[pid_dict[row['pid']], track_dict[row['track_uri']]] = 1\n",
    "    return result\n",
    "\n",
    "\n",
    "# create list of tuples where recommendation array should have a 1\n",
    "# i.e. if (2,18) is in this list, then playlist 2 contains track 18\n",
    "def make_R_list(df):\n",
    "    result = []\n",
    "    track_dict = make_track_dict(df)\n",
    "    pid_dict = make_pid_dict(df)\n",
    "    for index , row in df.iterrows():\n",
    "        result.append((pid_dict[row['pid']], track_dict[row['track_uri']]))\n",
    "    return result\n",
    "\n",
    "# create the R_list for grandma's hypothesis method.  As coded top 20% gets 2 instead of 1.\n",
    "def make_R_list_grandma(df):\n",
    "    result = []\n",
    "    track_dict = make_track_dict(df)\n",
    "    for index , row in df.iterrows():\n",
    "        if row['pos'] < 0.2 * row['num_tracks']:\n",
    "            result.append((row['pid'], track_dict[row['track_uri']], 2))\n",
    "        else: \n",
    "            result.append((row['pid'], track_dict[row['track_uri']], 1))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions for running gradient descent and computing the error function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use gradient descent to minimize MSE (with l2 regularization)\n",
    "def update_params_loop(R_list, P, Q, alpha, llambda):\n",
    "    newP = P\n",
    "    newQ = Q\n",
    "    m , f = np.shape(P)\n",
    "    n = np.shape(Q)[0]\n",
    "\n",
    "    for u,i in R_list:\n",
    "        newP[u,:] -= alpha * 2 * (P[u,:] @ Q[i,:] - 1) * Q[i,:]\n",
    "        newQ[i,:] -= alpha * 2 * (P[u,:] @ Q[i,:] - 1) * P[u,:]\n",
    "    for u in range(m):\n",
    "        newP[u,:] -= alpha * 2 * llambda * P[u,:]\n",
    "    for i in range(n):\n",
    "        newQ[i,:] -= alpha * 2 * llambda * Q[i,:]\n",
    "    return (newP, newQ)\n",
    "\n",
    "# use gradient descent with where R_list has triples (u,i,score)\n",
    "def update_params_loop_score(R_list, P, Q, alpha, llambda):\n",
    "    newP = P\n",
    "    newQ = Q\n",
    "    m , f = np.shape(P)\n",
    "    n = np.shape(Q)[0]\n",
    "\n",
    "    for u,i, score in R_list:\n",
    "        newP[u,:] -= alpha * 2 * (P[u,:] @ Q[i,:] - score) * Q[i,:]\n",
    "        newQ[i,:] -= alpha * 2 * (P[u,:] @ Q[i,:] - score) * P[u,:]\n",
    "    for u in range(m):\n",
    "        newP[u,:] -= alpha * 2 * llambda * P[u,:]\n",
    "    for i in range(n):\n",
    "        newQ[i,:] -= alpha * 2 * llambda * Q[i,:]\n",
    "    return (newP, newQ)\n",
    "\n",
    "# treat missing top 1000 songs as 0's and also include z random 0's per playlist\n",
    "def update_params_loop_zeros(R_list, P, Q, alpha, llambda, z):\n",
    "    newP = P\n",
    "    newQ = Q\n",
    "    m , f = np.shape(P)\n",
    "    n = np.shape(Q)[0]\n",
    "\n",
    "    u_current = 0\n",
    "    u_set = set()\n",
    "\n",
    "    for u,i in R_list:\n",
    "\n",
    "        # Do the usual gd update\n",
    "        if u_current == u: \n",
    "            newP[u,:] -= alpha * 2 * (P[u,:] @ Q[i,:] - 1) * Q[i,:]\n",
    "            newQ[i,:] -= alpha * 2 * (P[u,:] @ Q[i,:] - 1) * P[u,:]\n",
    "            u_set.add(i) # keep track of songs in the playlist\n",
    "        else: \n",
    "            rand_song_list = [random.randint(0,n-1) for a in range(z)] # Get list of z random songs\n",
    "            #rand_song_set = set(rand_song_list)\n",
    "\n",
    "            for song in rand_song_list: # Treat these random songs as 0's for this playlist and do gd update\n",
    "                if song not in u_set:\n",
    "                    newP[u,:] -= alpha * 2 * (P[u,:] @ Q[song,:]) * Q[song,:]\n",
    "                    newQ[song,:] -= alpha * 2 * (P[u,:] @ Q[song,:]) * P[u,:]\n",
    "            # for song in top_1000: # Treat songs missing from the top 1000 as 0's\n",
    "            #     if song not in u_set and song not in rand_song_set:\n",
    "            #         newP[u,:] -= alpha * 2 * (P[u,:] @ Q[song,:]) * Q[song,:]\n",
    "            #         newQ[song,:] -= alpha * 2 * (P[u,:] @ Q[song,:]) * P[u,:]\n",
    "\n",
    "            newP[u,:] -= alpha * 2 * (P[u,:] @ Q[i,:] - 1) * Q[i,:]\n",
    "            newQ[i,:] -= alpha * 2 * (P[u,:] @ Q[i,:] - 1) * P[u,:]\n",
    "            u_current = u\n",
    "            u_set = set([i])\n",
    "\n",
    "    # Regularization\n",
    "    for u in range(m):\n",
    "        newP[u,:] -= alpha * 2 * llambda * P[u,:]\n",
    "    for i in range(n):\n",
    "        newQ[i,:] -= alpha * 2 * llambda * Q[i,:]\n",
    "    return (newP, newQ)\n",
    "\n",
    "\n",
    "#runs the gradient descent loop in batches\n",
    "def gd_batch(R_list, P, Q, alpha, llambda, batch_num, iterations, R = None, verbose=False):\n",
    "    #make copies of P and Q\n",
    "    P_current = P.copy()\n",
    "    Q_current = Q.copy()\n",
    "\n",
    "    #shuffle R_list\n",
    "    #divide R_list into batch_num subsets\n",
    "    random.shuffle(R_list)\n",
    "    batch_size = int(np.ceil(len(R_list)/batch_num))\n",
    "    R_batch = [R_list[i:i+batch_size] for i in range(0,len(R_list), batch_size) ]\n",
    "\n",
    "    #loop over total iterations\n",
    "    for i in range(iterations):\n",
    "        #if verbose == true print out error function\n",
    "        if verbose:\n",
    "            print(f'Step {i*batch_num}: Error function={error_function(R_list,R , P_current, Q_current)}')\n",
    "        #loop over batch_num\n",
    "        for batch in R_batch:\n",
    "            #run update_param_loop on batch\n",
    "            P_current , Q_current = update_params_loop(batch, P_current, Q_current, alpha, llambda)\n",
    "\n",
    "    return (P_current , Q_current)\n",
    "\n",
    "#error function without l2 normalization factor\n",
    "def error_function( R_list,R, P , Q ):\n",
    "    result = 0\n",
    "    #sum over R_list\n",
    "    for row, col in R_list:\n",
    "        result = result + (R[row,col] - P[row,:]@Q[col,:])**2\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function for obtaining the playlist vector that minimizes the cost function for a fixed $Q$ from a list of track id's. Note, to use this, the song list must have at least $f$ songs where $f$ is the number of latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_user_vec(tid_list, Q, llambda):\n",
    "    Y = Q[tid_list,:]\n",
    "    f = np.shape(Q)[1]\n",
    "    d = len(tid_list)\n",
    "    vec = np.linalg.inv(np.transpose(Y) @ Y + llambda * np.identity(1)) @ np.transpose(Y) @ np.ones((d,1))\n",
    "    return np.transpose(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRfZHGotHyXh"
   },
   "source": [
    "# Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YhJsEpFdHyXi"
   },
   "outputs": [],
   "source": [
    "num_jsons = 20 # number of files to read\n",
    "data = read_data(num_jsons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_list = make_R_list(data)\n",
    "track_dict = make_track_dict(data)\n",
    "reverse_track_dict = make_reverse_track_dict(data)\n",
    "sorted_tid_list = make_sorted_tid_list(R_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 7 # number of latent features\n",
    "num_songs = max(reverse_track_dict.keys()) + 1\n",
    "num_playlists = num_jsons*1000\n",
    "\n",
    "\n",
    "# initialize random values for matrices P and Q. Entries are between -1 and 1\n",
    "P = np.random.rand(num_playlists, f) * 2 - 1\n",
    "Q = np.random.rand(num_songs, f) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "b_nyoRWcHyXi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 1, 2, 3, Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dane\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Dane\\AppData\\Local\\Temp\\ipykernel_24436\\2788256200.py\", line 7, in <module>\n",
      "    P_current , Q_current = update_params_loop_zeros(R_list, P_current, Q_current, 0.1, 0.25, 1000)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dane\\AppData\\Local\\Temp\\ipykernel_24436\\1334580116.py\", line -1, in update_params_loop_zeros\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dane\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 2142, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dane\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dane\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dane\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dane\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dane\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dane\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dane\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dane\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dane\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dane\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dane\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dane\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"C:\\Users\\Dane\\AppData\\Roaming\\Python\\Python311\\site-packages\\executing\\executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# Run the gradient descent algorithm\n",
    "P_current = P.copy()\n",
    "Q_current = Q.copy()\n",
    "for i in range(100):\n",
    "    if i % 1 == 0:\n",
    "        print(i, end = ', ')\n",
    "    P_current , Q_current = update_params_loop_zeros(R_list, P_current, Q_current, 0.1, 0.25, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juN8npsfHyXj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
