{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c56a9ca-c672-4d8d-a014-4b495f36d8e7",
   "metadata": {},
   "source": [
    "## Load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2da703-530a-4859-a17f-7816f564c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hashtag and run code below to install packages\n",
    "\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2bfaa2-2b3b-4567-b22d-64bf89c074aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import copy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2304a7f1-d7c5-4778-8f08-f150831880ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n is the number of json files we wish to read (max is 1000)\n",
    "# this sets n to randomly draw from the 1000 json files, with size indicating how many\n",
    "# json files you want\n",
    "\n",
    "# uncomment the following line to ensure you get the same json files when running the code\n",
    "#np.random.seed(110)\n",
    "n = np.random.randint(1001, size = 5)\n",
    "\n",
    "\n",
    "for i in (n):\n",
    "    file_name = '../spotify_million_playlist_dataset/data/mpd.slice.' \\\n",
    "    + str(i*1000) \\\n",
    "    + \"-\" + str(i*1000+999) + '.json'\n",
    "\n",
    "    #uncomment the following line to show progress\n",
    "    print(file_name)\n",
    "\n",
    "    #open the file and store its contents in file_contets\n",
    "    with open(file_name) as user_file:\n",
    "        file_contents = user_file.read()\n",
    "\n",
    "    # only care about the \"playlists\" part of dictionatry\n",
    "    # save the list of playlists in playlist_list\n",
    "    parsed_json = json.loads(file_contents)\n",
    "    playlist_list = parsed_json['playlists']\n",
    "\n",
    "    # create dataframe if it's first playlist, otherwise append info to existing dataframe\n",
    "    # the record_path argument tells the json_normalize function how to flatten the data\n",
    "    # the meta argument tells the json_normalize function what meta data to keep\n",
    "\n",
    "    if i == min(n):\n",
    "        data = pd.json_normalize(\n",
    "            playlist_list, \n",
    "            record_path = 'tracks', \n",
    "            meta = ['name', 'collaborative', 'pid', 'num_followers', 'num_edits']\n",
    "        )\n",
    "    else:\n",
    "        data = pd.concat([data,\n",
    "                          pd.json_normalize(\n",
    "                              playlist_list,\n",
    "                              record_path = 'tracks', \n",
    "                              meta = ['name', 'collaborative', 'pid', 'num_followers', 'num_edits']\n",
    "                          )\n",
    "                         ], \n",
    "                         ignore_index = True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bff9053-2f25-4b92-afec-5d7639baf853",
   "metadata": {},
   "source": [
    "## Clean Playlist Titles (Lemmmatize, lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bc4d5b-87ab-4f40-b2d3-787fe0fe3d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spacy object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# Function to lemmatize texts using nlp.pipe\n",
    "def text_processing(title):\n",
    "    # nlp.pipe processes texts as a stream\n",
    "    for doc in nlp.pipe(title, disable=[\"parser\", \"ner\"]):\n",
    "        yield ' '.join([token.lemma_.lower() for token in doc])\n",
    "\n",
    "# Apply the lemmatization function to the text column using nlp.pipe. \n",
    "# Adds new column \"playlist_title\" to dataframe\n",
    "data['playlist_title'] = list(text_processing(data['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446eb151-d812-45b0-a30d-2872313bf3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new version of the dataframe with duplicate playlist titles removed.\n",
    "# Since this operation is being done on the duplicate values of pid column, should not impact\n",
    "# playlist titles that now have same name. Note, this can be done before applying the text_processing\n",
    "# function. Just change the name of the reference dataframe accordingly.\n",
    "\n",
    "df = data.drop_duplicates(subset = [\"pid\"]).copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
